CartPole-v0:
  env: CartPole-v0
  max_epochs: 10
  batch_size: 32
  abstract_state_dim: 128
  lr_decay: 0.95
  optimizer:
    name: Adam
    params: { lr: !!python/float 1e-2 }
    # grad_clip_norm: 1.0
    # qnet_lr: !!python/float 5e-4
    # encoder_lr: !!python/float 5e-4
  gamma: 0.99
  sync_rate: 20
  replay_size: 1000
  warm_start_size: 1000
  eps_last_frame: 500
  eps_start: 1.0
  eps_end: 0.01
  max_episode_length: 200
  max_episode_reward: 200
  planning_depth: 0
  is_atari: False
  replace: True
  update_experience: True
  logger_dir: lightning_dirs/cartpole
  plot_dir: /home/aayn/Programming/yarl/plots/cartpole

PongNoFrameskip-v4:
  env: PongNoFrameskip-v4
  max_epochs: 50000
  batch_size: 32
  abstract_state_dim: 100
  lr_decay: 0.95
  optimizer:
    name: Adam
    params: { lr: !!python/float 1e-4 }
    # qnet_lr: !!python/float 5e-4
    # encoder_lr: !!python/float 5e-4
  gamma: 0.99
  sync_rate: 1000
  replay_size: 10000
  warm_start_size: 10000
  eps_last_frame: 50000
  eps_start: 1.0
  eps_end: 0.01
  max_episode_length: 5000
  max_episode_reward: 21
  planning_depth: 0
  is_atari: True
  replace: True
  update_experience: True
  logger_dir: lightning_dirs/pong

SimpleMaze-v0:
  env: SimpleMaze-v0
  max_epochs: 500
  batch_size: 32
  abstract_state_dim: 2
  lr: !!python/float 5e-4
  lr_decay: 0.95
  optimizer:
    name: RMSprop
    params: { lr: !!python/float 5e-4 }
    grad_clip_norm: 1.0
    # qnet_lr: !!python/float 5e-4
    # encoder_lr: !!python/float 5e-4
  gamma: 0.99
  sync_rate: 1000 # freeze interval
  replay_size: 5000
  warm_start_size: 5000
  eps_last_frame: 10000
  eps_start: 1.0
  eps_end: 1.0 # Random actions
  max_episode_length: 2000
  max_episode_reward: 0
  planning_depth: 0
  is_atari: False
  replace: True
  update_experience: False
  logger_dir: lightning_dirs/simple_mazev0
  plot_dir: /home/aayn/Programming/yarl/plots/test_refactor1

SimpleMaze-v1:
  env: SimpleMaze-v1 # OpenAI Gym env id
  alg_name: Prioritized-Dueling-DDQN
  max_epochs: 50000
  batch_size: 32
  abstract_state_dim: 128
  lr: !!python/float 5e-4
  lr_decay: 1.0
  optimizer:
    name: Adam
    params: { lr: !!python/float 5e-4 }
    grad_clip_norm: 10.0
    # qnet_lr: !!python/float 5e-4
    # encoder_lr: !!python/float 5e-4
  gamma: 0.95 # discount factor
  sync_rate: 200 # freeze interval
  replay_size: 10000 # replay memory size
  warm_start_size: 2000
  eps_last_frame: 2000
  eps_start: 1.0
  eps_end: 0.01
  max_episode_length: 1000
  max_episode_reward: 100
  planning_depth: 0
  is_atari: False
  replace: True # whether to sample from experience with replacement
  update_experience: True # whether to store new experience into replay buffer
  prioritized_buffer: True # whether to use prioritized experience replay
  prioritization_alpha: 0.7
  prioritization_beta: 0.7
  logger_dir: lightning_dirs/simple_mazev1
  plot_dir: /home/aayn/Programming/yarl/plots/test_goal_plots18_planning2_interp
